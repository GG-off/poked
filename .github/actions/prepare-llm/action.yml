name: Prepare TinyLlama
description: Download TinyLlama and launch it
runs:
  using: "composite"
  steps:
    - name: Download TinyLlama model
      shell: bash
      run: curl -OL https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

    - name: Download and unpack llama.cpp
      shell: bash
      run: |
        curl -OL https://github.com/ggerganov/llama.cpp/releases/download/b2751/llama-b2751-bin-ubuntu-x64.zip
        unzip llama-b2751-bin-ubuntu-x64.zip

    - name: Launch llama.cpp
      shell: bash
      run: ./build/bin/server -m ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf &

    - name: Wait until it is ready
      shell: bash
      run: while ! curl -s 'http://localhost:8080/health' | grep 'ok'; do sleep 1; done
